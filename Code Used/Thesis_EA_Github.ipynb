{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55f108b-a6c0-4ad2-9acb-b3b608a7f2f4",
   "metadata": {},
   "source": [
    "## Error analysis of sequencing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b8c09-21a8-49d9-b55e-083b7ea5b00c",
   "metadata": {},
   "source": [
    "Here Biopython is imported to parse sequencing data from SAM files that were generated using bwa mem and to use its gc fraction function. \\\n",
    "Numpy is used to help perform calculations on arrays instead of parsing lists. \\\n",
    "Pandas is used to format results into tables and export as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "867e3248-aa63-4269-9c50-b044fa7f845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import gc_fraction\n",
    "from Bio import Align\n",
    "import pysam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab747464-fc05-4378-a66f-2a8a3def25f4",
   "metadata": {},
   "source": [
    "#### Q-score functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b65303-0e69-47ef-a78c-030a47e6040e",
   "metadata": {},
   "source": [
    "The function below grabs the index of each hompolymer of at least the minimum size. Then simply gets the quality score of these indexes (homopolymers) to get the average, as well as calculating the overall average q-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2734bc2a-2729-42f4-bf0b-0f9cb999aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polymer_avg_qscore(recseq, plength = 3, downstream = 1, skip = 2, trim = 0, type = None):\n",
    "    \"\"\"\n",
    "    A function that anaylses the q-score within hompolymers,\n",
    "    returns the average q-score within homopolymers and the overall average q-score.\n",
    "    recseq - is a seqrecord object that must contain quality scores\n",
    "    plength - int, length (equal or grater than) of homopolymers to be detected. Default = 3\n",
    "    downstram - int, how large a margin in bases to look for. For example, if 0 then LQB must.\n",
    "                be within the homopolymer, if 1 can be within homopolymer and one bp downstream.\n",
    "                Default = 1\n",
    "    skip - int, number of bp to skip at the beginning of the homopolymer when looking for LQB.\n",
    "           default = 2\n",
    "    trim - float, percentage (0 to 1) of start and tail of sequence removed prior to counting,\n",
    "           a value of 0 results in no trimming. Default = 0.1\n",
    "    type - None, \"AT\" or \"GC\", type of homopolymers to look for,\n",
    "           None searches for all \n",
    "    \"\"\"\n",
    "    #Trim sequence\n",
    "    recseq = recseq[int(trim*len(recseq.seq)):int((1-trim)*len(recseq.seq))]\n",
    "    #Index of beginning of homopolymers\n",
    "    if type == None:\n",
    "        idx_list = [idx for idx,i in recseq.seq.search([\"A\"*plength, \"T\"*plength, \"G\"*plength, \"C\"*plength])]\n",
    "    elif type == \"AT\":\n",
    "        idx_list = [idx for idx,i in recseq.seq.search([\"A\"*plength, \"T\"*plength])]\n",
    "    elif type == \"GC\":\n",
    "        idx_list = [idx for idx,i in recseq.seq.search([\"G\"*plength, \"C\"*plength])]\n",
    "    else:\n",
    "        raise ValueError(\"type should be None 'AT' or 'GC'\")\n",
    "\n",
    "    if len(idx_list) > 0:\n",
    "        idx_list.append(idx_list[-1] + 100) #dummy value as the last element in the loop below is ignored.\n",
    "    new_idx_list = []\n",
    "    area = []\n",
    "    cycle = 0 #To distinguish start and end\n",
    "    # This section here combines hompolymer run indexes\n",
    "    for i in range(len(idx_list)):\n",
    "        if i == 0: #skip first index\n",
    "            continue\n",
    "            \n",
    "        if ((idx_list[i-1] + plength) > idx_list[i]):\n",
    "            if cycle == 0:\n",
    "                cycle = 1\n",
    "                area.append(idx_list[i-1])\n",
    "        else:\n",
    "            if cycle == 1:\n",
    "                area.append(idx_list[i-1])\n",
    "                new = list(range(area[0]+skip,area[1]+plength+downstream))\n",
    "                new_idx_list.append(new)\n",
    "                area = []\n",
    "                cycle = 0\n",
    "            else:\n",
    "                new = list(range(idx_list[i-1]+skip, idx_list[i-1]+plength+downstream)) \n",
    "                new_idx_list.append(new)\n",
    "\n",
    "    qscore = 0\n",
    "    pol_length = 0\n",
    "    for i in new_idx_list:\n",
    "        for j in i:\n",
    "            #catching occurences where a hompolymer occurs at the end of a read\n",
    "            if j == len(recseq.seq):\n",
    "                continue\n",
    "            pol_length += 1\n",
    "            qscore += recseq.letter_annotations[\"phred_quality\"][j]\n",
    "    if pol_length == 0:\n",
    "        return(0, (sum(recseq.letter_annotations[\"phred_quality\"]) / len(recseq.seq)))\n",
    "    else:\n",
    "        return ((qscore/pol_length), (sum(recseq.letter_annotations[\"phred_quality\"]) / len(recseq.seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc866c-8478-41f3-920c-6fea333d897f",
   "metadata": {},
   "source": [
    "This function analyses the average q-score of a certain windowsize and records the gc content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59e62596-581e-47f1-8635-6f5a500631d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_gc_qscore(recseq, windowsize, step, trim = 0):\n",
    "    \"\"\"\n",
    "    A function that performs a sliding window analysis of GC conent.\n",
    "    Calculates the GC fraction of each window and its average Q-score\n",
    "    Returns two lists, the first containing gc fractions and the second\n",
    "    containing the Q-score.\n",
    "    recseq - is a seqrecord object that must contain quality scores.\n",
    "    qscore - int, any q-score equal to or less than this value will be \n",
    "             treated as a LQB.\n",
    "    windowsize - int, the size of the sliding window\n",
    "    step - int, the number of bases the window slides by through each iteration.\n",
    "    trim - float, percentage (0 to 1) of start and tail of sequence removed\n",
    "           prior to counting, a value of 0 results in no trimming. Default = 0.1\n",
    "    \"\"\"\n",
    "    recseq = recseq[int(trim*len(recseq.seq)):int((1-trim)*len(recseq.seq))]\n",
    "    q_list = recseq.letter_annotations[\"phred_quality\"]\n",
    "\n",
    "    if windowsize > len(recseq):\n",
    "        return([(gc_fraction(recseq.seq), (sum(q_list) / len(q_list)))])\n",
    "\n",
    "    qscores = []\n",
    "    i = 0\n",
    "    while (i + windowsize) < len(recseq):\n",
    "\n",
    "        sequ = recseq[i:i+windowsize]\n",
    "        gc = gc_fraction(sequ.seq)\n",
    "        lqb_count = 0\n",
    "        avgscore = sum(q_list[i:i+windowsize]) / len(sequ)\n",
    "        qscores.append((gc,avgscore))\n",
    "        i = i + step\n",
    "\n",
    "    return(qscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b610b973-c0a0-4a4f-bdae-3d6937fee775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_period(s):\n",
    "    \"\"\"\n",
    "    A function that detects if a string in its entirety consists of a repeating pattern.\n",
    "    Returns a string of the repeating pattern.\n",
    "    E.g. \"ATGATG\" returns \"ATG\", \"ATGATGT\" returns None\n",
    "    \"\"\"\n",
    "    i = (s+s).find(s, 1, -1)\n",
    "    return None if i == -1 else s[:i]\n",
    "\n",
    "def find_repeats_qscore(recseq, repeat_length = 3, ignore = 0, downstream = 1, trim = 0):\n",
    "    \"\"\"\n",
    "    A function that finds the average score of the bases in a repeat and\n",
    "    a specified number of bases downstream of the repeat.\n",
    "    Returns the average score across these bases.\n",
    "    recseq - is a seqrecord object that must contain quality scores.\n",
    "    repeat_length - the size of the repeating pattern. Default = 3\n",
    "    ignore - how many bases to ignore at the start of the repeat. Default = 0\n",
    "             for example, a value of 2, with \"ATGATG\", only \"GATG\" is counted.\n",
    "    downstram - int, how many bases downstream to include in the calculation.\n",
    "                for example, value of 1 means in \"ATGATGC\" the C will be taken\n",
    "                into account while 0 means only the repeat \"ATGATG\" will be taken\n",
    "                into account. Default = 1\n",
    "    trim - float, percentage (0 to 1) of start and tail of sequence removed prior to counting,\n",
    "           a value of 0 results in no trimming. Default = 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    #Overlapping repeats are treated individually. E.g TATATAATA has TATAT and ATAATA overlapping.\n",
    "    #Means more emphasis on these regions.\n",
    "    q_list = recseq.letter_annotations[\"phred_quality\"] \n",
    "    sequ = recseq[int(trim*len(recseq.seq)):int((1-trim)*len(recseq.seq))]\n",
    "    idx_list = []\n",
    "    previous = \"\"\n",
    "    #This section here creates a list of lists of the start and end indexes\n",
    "    # of each repeat. E.g [[201,206], [353,358]...]\n",
    "    for i in range(repeat_length): #For each reading frame\n",
    "        while (i+(repeat_length*2)) < len(sequ):\n",
    "            x = sequ[i:i+(repeat_length*2)].seq #Sliding window\n",
    "            pat = principal_period(x)\n",
    "            if pat is not None:\n",
    "                if len(pat) == repeat_length: #If pattern present\n",
    "                    if pat == previous: #If repeat is continuing, edit previous end index.\n",
    "                        idx_list[-1][1] = idx_list[-1][1] + repeat_length\n",
    "                    else: #Else add new index range.\n",
    "                        idx_list.append([i,i+(repeat_length*2)])\n",
    "                        previous = pat\n",
    "            \n",
    "            i += repeat_length\n",
    "\n",
    "    #NOTE: probably possible to remove the whole sliding window by repeat size and just\n",
    "    #      have it always increment by 1.\n",
    "    \n",
    "    q_scores = []\n",
    "    for i in idx_list:\n",
    "        # if statement to avoid out of index range errors\n",
    "        if (i[1] + downstream) >= len(q_list):\n",
    "            q_scores = q_scores + q_list[i[0]:i[1]]\n",
    "        else:\n",
    "            q_scores = q_scores + q_list[(i[0] + ignore):(i[1] + downstream)]\n",
    "\n",
    "    if len(q_scores) == 0:\n",
    "        return 0\n",
    "    return(sum(q_scores) / len(q_scores))         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98558cb-1779-4107-8d7a-7470ea7886d6",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b4372-4669-4b80-b4e2-f319de2566ef",
   "metadata": {},
   "source": [
    "#### Indel/substitution analysis\n",
    "In contrast to the earlier Q-score analysis, this now relies on SAM files to detect indels, transitions and transversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c8fa1-9c76-4c13-9937-da9e53b8854e",
   "metadata": {},
   "source": [
    "This simply checks if a particular string has a polymer (used later). Can also check for only AT or GC polymers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cae7df5f-65f7-4f48-a59a-eb5ed6fd84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_polymer(sequ, length = 3, type = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function that checks if a sequence contains a homopolymer of certain length.\n",
    "    length - int, minimum length of hompolymer\n",
    "    type - None, \"AT\" or \"GC\", type of homopolymers to look for\n",
    "           None searches for all \n",
    "    \"\"\"\n",
    "\n",
    "    if type == None:\n",
    "        if len(sequ) < length:\n",
    "            print('Error sequence less than homopolymer length')\n",
    "            return\n",
    "        elif sequ.count(\"A\"*length) != 0:\n",
    "            return True\n",
    "        elif sequ.count(\"T\"*length) != 0:\n",
    "            return True\n",
    "        elif sequ.count(\"G\"*length) != 0:\n",
    "            return True\n",
    "        elif sequ.count(\"C\"*length) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif type == \"AT\":\n",
    "        if len(sequ) < length:\n",
    "            print('Error sequence less than homopolymer length')\n",
    "            return\n",
    "        elif sequ.count(\"A\"*length) != 0:\n",
    "            return True\n",
    "        elif sequ.count(\"T\"*length) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif type == \"GC\":\n",
    "        if len(sequ) < length:\n",
    "            print('Error sequence less than homopolymer length')\n",
    "            return\n",
    "        elif sequ.count(\"G\"*length) != 0:\n",
    "            return True\n",
    "        elif sequ.count(\"C\"*length) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        raise ValueError(\"Type must be None, 'AT' or 'GC'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5486e8-0f05-421a-bf5f-4ac8c334aff7",
   "metadata": {},
   "source": [
    "The function below is used because the original biopython parse function doesn't ignore failed alignments and throws an error. \\\n",
    "The failed alignments are removed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a21be42c-81f7-4e44-aa43-e9f0e8a6e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_iterator(alignments):\n",
    "    '''\n",
    "    Some alignments fail to be initiated, which crashes the biopython\n",
    "    iterator with no recourse. Here we catch these errors and just \n",
    "    skip them. Returns a list of all alignments. (Alignment objects)\n",
    "    alginments - AlignmentIterator Object\n",
    "    '''\n",
    "    ret = []\n",
    "    while True:\n",
    "        try:\n",
    "            x = next(alignments)\n",
    "        except TypeError: #Catch failed initialization\n",
    "            continue\n",
    "        except StopIteration: # Raised at end of iterator, stop here.\n",
    "            break\n",
    "        else:\n",
    "            ret.append(x)\n",
    "    return(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e8267-f3e7-45b3-9132-a38d7fa74066",
   "metadata": {},
   "source": [
    "The class below is set up to store results in an easily accessable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba630c69-e42d-4bf5-b5f5-f862908b4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignResults:\n",
    "    \"\"\"\n",
    "    A class to store results from alignment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alignment, gaps, ident, mismatches, transitions, transversions, insertions, deletions,\n",
    "                gap_idx, iden_idx, mis_idx, ts_idx, tv_idx, ins_idx, del_idx):\n",
    "\n",
    "        self.alignment = alignment\n",
    "        self.gaps = gaps\n",
    "        self.identities = ident\n",
    "        self.mismatches = mismatches\n",
    "        self.transitions = transitions\n",
    "        self.transversions = transversions\n",
    "        self.insertions = insertions\n",
    "        self.deletions = deletions\n",
    "        self.index_gaps = gap_idx\n",
    "        self.index_identities = iden_idx\n",
    "        self.index_mismatches = mis_idx\n",
    "        self.index_transitions = ts_idx\n",
    "        self.index_transversions = tv_idx\n",
    "        self.index_insertions = ins_idx\n",
    "        self.index_deletions = del_idx\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return(f\"AlignResults Class, {self.gaps}, {self.identities}, {self.mismatches}, {self.transitions}, {self.transversions}, {self.insertions}, {self.deletions}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alignment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965c2f8-5fcf-46ec-a4a8-63853d4bcc0e",
   "metadata": {},
   "source": [
    "The function below is the main function in this analysis. It analyses a sequence, counts indels, transitions and transversions as well as storing the index of their locations in the alignresults class. \\\n",
    "The information stored here is used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0329ef8d-a40a-422f-9027-752fc83ffbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_analysis(alignment, show_errors = False, cutoff = None):\n",
    "    \"\"\"\n",
    "    A function that performs analyses on an alignment object.\n",
    "    Creates an AlignResults objects which contain the results.\n",
    "    Counts insertions, deletions, mismatches (substitutions),\n",
    "    gaps, indetities, transitions, transversions and their\n",
    "    indexes.\n",
    "    \"\"\"\n",
    "\n",
    "    #Sometimes an invalid alignment is created, this skips them\n",
    "    try:\n",
    "        x = alignment[1]\n",
    "    except ValueError:\n",
    "        if show_errors == True:\n",
    "            print(\"Invalid alignment object encountered, skipping...\")\n",
    "        return\n",
    "    ref = alignment[0]\n",
    "    gen = alignment[1]\n",
    "    gaps = 0\n",
    "    identities, mismatches = 0,0\n",
    "    transitions, transversions = 0,0\n",
    "    insertions, deletions = 0,0\n",
    "\n",
    "    gap_idx, iden_idx, mis_idx = [],[],[]\n",
    "    ts_idx, tv_idx, ins_idx, del_idx = [],[],[],[]\n",
    "\n",
    "    if len(ref) != len(gen):\n",
    "        print(\"Alignments not same length\")\n",
    "        return\n",
    "\n",
    "    for i in range(len(ref)):\n",
    "        if ref[i] == \"-\":\n",
    "            insertions += 1\n",
    "            gaps += 1\n",
    "            gap_idx.append(i)\n",
    "            ins_idx.append(i)\n",
    "        elif gen[i] == \"-\":\n",
    "            deletions += 1\n",
    "            gaps += 1\n",
    "            gap_idx.append(i)\n",
    "            del_idx.append(i)\n",
    "        elif ref[i] == gen[i]:\n",
    "            identities += 1\n",
    "            iden_idx.append(i)\n",
    "        elif ref[i] != gen[i]:\n",
    "            mismatches += 1\n",
    "            mis_idx.append(i)\n",
    "            if gen[i] in [\"C\",\"T\"]:\n",
    "                if ref[i] in [\"C\",\"T\"]:\n",
    "                    transitions += 1\n",
    "                    ts_idx.append(i)\n",
    "                else:\n",
    "                    transversions += 1\n",
    "                    tv_idx.append(i)\n",
    "            else:\n",
    "                if ref[i] in [\"A\",\"G\"]:\n",
    "                    transitions += 1\n",
    "                    ts_idx.append(i)\n",
    "                else:\n",
    "                    transversions += 1\n",
    "                    tv_idx.append(i)\n",
    "\n",
    "    if cutoff != None:\n",
    "        if (gaps + mismatches) > (len(ref) * cutoff):\n",
    "            return\n",
    "        \n",
    "    result = AlignResults(alignment, gaps, identities, mismatches, transitions, transversions, insertions, deletions, gap_idx, iden_idx, mis_idx, ts_idx, tv_idx, ins_idx, del_idx)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27db55c-794a-4af5-a2d7-6d99e87f8328",
   "metadata": {},
   "source": [
    "This simply gets the flat error rate for each type of error within a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c66a7256-8ed3-48e8-b75d-48aa90df4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_error_analysis(alignres, group = True):\n",
    "    \"\"\"\n",
    "    A function that returns the per base transition rate\n",
    "    transversion rate, insertion rate and deletion rate.\n",
    "    alignres - An AlignResults object or list containing\n",
    "               AlignResults objects.\n",
    "    group - boole, whether or not to group indels together.\n",
    "            For example \"TG---CT\" would be 3 gaps if group\n",
    "            is False, or would be 1 gap if group is True.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(alignres) == AlignResults:\n",
    "        if group == False:\n",
    "            size = len(alignres.alignment[0])\n",
    "            return(alignres.transitions / size, alignres.transversions / size, alignres.insertions / size, alignres.deletions / size)\n",
    "        else:\n",
    "            size = len(alignres.alignment[0])\n",
    "            insert = len(area_creator(alignres.index_insertions))\n",
    "            delet = len(area_creator(alignres.index_deletions))\n",
    "            return(alignres.transitions / size, alignres.transversions / size, insert / size, delet / size)\n",
    "    \n",
    "    elif type(alignres) == list:\n",
    "        ts, tv, ins, dels, size = 0,0,0,0,0\n",
    "        if group == False:\n",
    "            for i in alignres:\n",
    "                if i != None:\n",
    "                    ts += i.transitions\n",
    "                    tv += i.transversions\n",
    "                    ins += i.insertions\n",
    "                    dels += i.deletions\n",
    "                    size += len(i.alignment[0])\n",
    "        else:\n",
    "            for i in alignres:\n",
    "                if i != None:\n",
    "                    ts += i.transitions\n",
    "                    tv += i.transversions\n",
    "                    ins += len(area_creator(i.index_insertions))\n",
    "                    dels += len(area_creator(i.index_deletions))\n",
    "                    size += len(i.alignment[0])\n",
    "        if size == 0:\n",
    "            size = 1\n",
    "        return([ts/size, tv/size, ins/size, dels/size])\n",
    "        \n",
    "    else:\n",
    "        raise TypeError(\"Must be an AlignResults class object or List of AlignResult class objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39a8dddb-b82c-453f-b203-05498522b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indel_size_analysis(alignres, drop_last_insertion = True, drop_limit = 20):\n",
    "    \"\"\"\n",
    "    A function that measures the size of each indel in a sequence.\n",
    "    Returns two lists, [insertion sizes], [deletion sizes].\n",
    "    alignres - An AlignResults object or list containing\n",
    "               AlignResults objects.\n",
    "    drop_last_insertion - Bool, Ignore the last insertion in a read if its \n",
    "                          size is above the drop limit. Default = True\n",
    "    drop_limit - int, size limit for last insertion\n",
    "    \"\"\"\n",
    "\n",
    "    if type(alignres) == AlignResults:\n",
    "        ins_sizes, del_sizes = [],[]\n",
    "        #Here we simply count the sizes of each indel and append it\n",
    "        #to a list\n",
    "        for i in area_creator(alignres.index_deletions):\n",
    "            if len(i) == 1:\n",
    "                del_sizes.append(1)\n",
    "            else:\n",
    "                del_sizes.append((i[1]-i[0])+1)\n",
    "        for i in area_creator(alignres.index_insertions):\n",
    "            if len(i) == 1:\n",
    "                ins_sizes.append(1)\n",
    "            else:\n",
    "                ins_sizes.append((i[1]-i[0])+1)\n",
    "        #If drop is True, we drop the last insertion if it\n",
    "        #is too large, this is to get rid of large insertions\n",
    "        #generated by the aligner at the end of reads.\n",
    "        if drop_last_insertion == True:\n",
    "            if len(ins_sizes) > 1:\n",
    "                 if ins_sizes[-1] > drop_limit:\n",
    "                     del ins_sizes[-1]\n",
    "        return(ins_sizes, del_sizes)\n",
    "\n",
    "    elif type(alignres) == list:\n",
    "        ins_sizes, del_sizes = [],[]\n",
    "        for align in alignres:\n",
    "            if align != None:\n",
    "                for i in area_creator(align.index_deletions):\n",
    "                    if len(i) == 1:\n",
    "                        del_sizes.append(1)\n",
    "                    else:\n",
    "                        del_sizes.append((i[1]-i[0])+1)\n",
    "                for i in area_creator(align.index_insertions):\n",
    "                    if len(i) == 1:\n",
    "                        ins_sizes.append(1)\n",
    "                    else:\n",
    "                        ins_sizes.append((i[1]-i[0])+1)\n",
    "                if drop_last_insertion == True:\n",
    "                    if len(ins_sizes) > 0:\n",
    "                        if ins_sizes[-1] > drop_limit:\n",
    "                            del ins_sizes[-1]\n",
    "        return(ins_sizes, del_sizes)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Must be an AlignResults class object or List of AlignResult class objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34f2a354-8372-44b6-ae1e-4d3f7a8177c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First find the homopolymer, then error rate within homopolymer and directly post homopolymer.\n",
    "\n",
    "def homopolymer_finder(alignres, polymer_length = 3, bases = None):\n",
    "    \"\"\"\n",
    "    A function that finds the indexes of all the\n",
    "    homopolymers in a sequence.\n",
    "    Returns a list of start and end indexes of each polymer.\n",
    "    alginres - AlignResults object or a DNA sequence string.\n",
    "    polymer_length - int, minumum homopolymer length.\n",
    "    bases - None, \"AT\" or \"GC\", type of homopolymers to look for\n",
    "            None searches for all \n",
    "    \"\"\"\n",
    "\n",
    "    #Takes in AlignResults object or a pure sequence.\n",
    "    if type(alignres) == AlignResults:\n",
    "        seq = alignres.alignment[0]\n",
    "    else:\n",
    "        seq = alignres\n",
    "\n",
    "    polymer_list = []\n",
    "    prev = False\n",
    "    memory, i = 0, 0\n",
    "    while (i+polymer_length-1) < len(seq):\n",
    "        if has_polymer(seq[i:i+polymer_length], polymer_length, bases) == True:\n",
    "            if prev == True:\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                prev = True\n",
    "                memory = i\n",
    "        else:\n",
    "            if prev == True:\n",
    "                prev = False\n",
    "                polymer_list.append([memory, i+polymer_length-2])\n",
    "        i+=1\n",
    "    #catch any homopolymers that are at the end of the sequence\n",
    "    if prev == True:\n",
    "        polymer_list.append([memory, i+polymer_length-2])\n",
    "    return(polymer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "859cb939-1664-4387-ae1f-eb4ac870195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homopolymer_analysis(alignres, polymer_length = 3, bases = None, specific = False):\n",
    "    \"\"\"\n",
    "    A function that analyses the error rates due to homopolymers.\n",
    "    It separately analyses error rates within the homopolymer and\n",
    "    error rate directly one base downstream of the homopolymer.\n",
    "    Returns two lists, [homopolymer_errors], [post-polymer errors]\n",
    "    which each contain four variables representing the error rate\n",
    "    of: transitions, transversions, insertions, deletions.\n",
    "\n",
    "    alignres - AlignResults object\n",
    "    polymer_length - int, minimum homopolymer length.\n",
    "    bases - None, \"AT\" or \"GC\", type of homopolymers to look for\n",
    "            None searches for all \n",
    "    specfic - Boole, if set to True, only hompolymers of the specified\n",
    "              length will be analysed rather than the default of specified\n",
    "              length or greater.\n",
    "    \"\"\"\n",
    "    polymer_indexes = homopolymer_finder(alignres, polymer_length, bases)\n",
    "    sequ = alignres.alignment[0]\n",
    "    #Keeping only homopolymers of certain length.\n",
    "    #TODO - clean this up\n",
    "    if specific == True:\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i >= len(polymer_indexes):\n",
    "                break\n",
    "            if polymer_indexes[i][1]-polymer_indexes[i][0] != polymer_length-1:\n",
    "                del polymer_indexes[i]\n",
    "            else:\n",
    "                i+=1\n",
    "                \n",
    "    #here errors follows [transitions,transverstions,\n",
    "    #                     insertions, deletions]\n",
    "    poly_errors = [0,0,0,0]\n",
    "    post_errors = [0,0,0,0]\n",
    "    poly_total, post_total = 0,0\n",
    "    for i in polymer_indexes:\n",
    "        for j in range(i[0], i[1]+1):\n",
    "            poly_total += 1\n",
    "            if j in alignres.index_transitions:\n",
    "                poly_errors[0] += 1\n",
    "            elif j in alignres.index_transversions:\n",
    "                poly_errors[1] += 1\n",
    "            #Technically this shouldn't be here as if it works\n",
    "            #correctly you shouldn't be getting any insertions\n",
    "            #mid polymer.\n",
    "            elif j in alignres.index_insertions:\n",
    "                poly_errors[2] += 1\n",
    "            elif j in alignres.index_deletions:\n",
    "                poly_errors[3] += 1\n",
    "\n",
    "        post_total += 1\n",
    "        if (i[1]+1) in alignres.index_transitions:\n",
    "            post_errors[0] += 1\n",
    "        elif (i[1]+1) in alignres.index_transversions:\n",
    "            post_errors[1] += 1\n",
    "        elif (i[1]+1) in alignres.index_insertions:\n",
    "            post_errors[2] += 1\n",
    "        elif (i[1]+1) in alignres.index_deletions:\n",
    "            post_errors[3] += 1\n",
    "\n",
    "    #Divide every element by the total\n",
    "    if poly_total != 0:\n",
    "        poly_errors[:] = [x / poly_total for x in poly_errors]\n",
    "    if post_total != 0: \n",
    "        post_errors[:] = [x / post_total for x in post_errors]\n",
    "    return (poly_errors, post_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21851084-cb56-4ad4-a56d-b8a62608e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homopolymer_analysis_multiple(results, polymer_length = 3, bases = None, specific = False):\n",
    "    \"\"\"\n",
    "    A function that analyses the error rates due to homopolymers.\n",
    "    It separately analyses error rates within the homopolymer and\n",
    "    error rate directly one base downstream of the homopolymer.\n",
    "    Returns two lists, [homopolymer_errors], [post-polymer errors]\n",
    "    which each contain four variables representing the error rate\n",
    "    of: transitions, transversions, insertions, deletions.\n",
    "\n",
    "    results - A list of AlignRes objects.\n",
    "    polymer_length - int, minimum homopolymer length.\n",
    "    bases - None, \"AT\" or \"GC\", type of homopolymers to look for\n",
    "            None searches for all.\n",
    "    specfic - Boole, if set to True, only hompolymers of the specified\n",
    "              length will be analysed rather than the default of specified\n",
    "              length or greater.\n",
    "    \"\"\"\n",
    "    \n",
    "    poly_errors = [0,0,0,0]\n",
    "    post_errors = [0,0,0,0]\n",
    "    count = 0\n",
    "    for i in results:\n",
    "        count += 1\n",
    "        if i != None:\n",
    "            x,y = homopolymer_analysis(i, polymer_length, bases, specific)\n",
    "            #Adding to existing errors\n",
    "            for j in range(0,4):\n",
    "                poly_errors[j] += x[j]\n",
    "                post_errors[j] += y[j]\n",
    "\n",
    "    if count == 0:\n",
    "        return([0,0,0,0],[0,0,0,0])\n",
    "    poly_errors[:] = [x / count for x in poly_errors]\n",
    "    post_errors[:] = [x / count for x in post_errors]\n",
    "\n",
    "    return(poly_errors, post_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "378fd3b4-b3b6-4c96-a081-6088f4dae0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_finder(alignres, repeat_length = 3, repeat_count = 2):\n",
    "    \"\"\"\n",
    "    A function that finds the indexes of all the\n",
    "    repeats in a sequence.\n",
    "    In this case the amount of repeats needing to occur\n",
    "    is only 2, so \"ATGATG\" will be included.\n",
    "    Returns a list of start and end indexes of each polymer.\n",
    "    alginres - AlignResults object or a DNA sequence string.\n",
    "    repeating_element - int, the size of the repeating element,\n",
    "                        e.g., 2 = \"ATAT\" - \"AT\",\n",
    "                              3 = \"ATGATG\" - \"ATG\".\n",
    "                        Default = 3\n",
    "    repeat_count - int, minimum times repeating element must\n",
    "                   be encountered. E.g., if 3 then\n",
    "                   \"ATGATG\" won't be taken into account but\n",
    "                   \"ATGATGATG\" will. Default = 2\n",
    "    \"\"\"\n",
    "\n",
    "    #Takes in AlignResults object or a pure sequence.\n",
    "    if type(alignres) == AlignResults:\n",
    "        sequ = alignres.alignment[0]\n",
    "    else:\n",
    "        sequ = alignres\n",
    "\n",
    "    repeat_idx = []\n",
    "    #This loop returns the starting index of each repeat.\n",
    "    #This does result in overlaps for repeat sizes greater than 2.\n",
    "    #But this is dealt with in the next loop\n",
    "    for i in range(repeat_length):\n",
    "        while (i+(repeat_length*2)) < len(sequ):\n",
    "            x = sequ[i:i+(repeat_length*2)]\n",
    "            pp = principal_period(x)\n",
    "            if pp is not None:\n",
    "                if len(pp) == repeat_length:\n",
    "                    repeat_idx.append(i)\n",
    "            i += repeat_length\n",
    "            \n",
    "    #Removing duplicating repeats\n",
    "    #For example \"ATG ATG ATG\"\n",
    "    #also contains \"A TGATGA TG\" \"TGA\" repeat\n",
    "    #Solve this by checking for overlaps and deleting them\n",
    "    repeat_idx.sort()\n",
    "    i = 0\n",
    "    while i < (len(repeat_idx)-1):\n",
    "        if i == 0:\n",
    "            i+=1\n",
    "            continue\n",
    "        if (repeat_idx[i] - repeat_idx[i-1]) < repeat_length:\n",
    "            del repeat_idx[i]\n",
    "            continue \n",
    "        i+=1\n",
    "\n",
    "    #This here converts the starting index of each repeat\n",
    "    #into a list of ranges as well as accounting for\n",
    "    #repeats where the repeating elements occurs more than twice.\n",
    "    combined_idx = []\n",
    "    prev = False #Are we currently in series or not\n",
    "    count = 0\n",
    "    for i in range(len(repeat_idx)):\n",
    "        #catch out of bounds\n",
    "        if i == len(repeat_idx)-1:\n",
    "            if prev == True:\n",
    "                prev = False\n",
    "                combined_idx.append([memory, repeat_idx[i]])\n",
    "            break\n",
    "        #If two repeats are in series:\n",
    "        if (repeat_idx[i+1] - repeat_idx[i]) == repeat_length:\n",
    "            if prev == False:\n",
    "                count = 1\n",
    "                prev = True\n",
    "                memory = repeat_idx[i]\n",
    "            else:\n",
    "                count+=1\n",
    "                continue\n",
    "        else:\n",
    "            if prev == True:\n",
    "                prev = False\n",
    "                if count >= (repeat_count-2):\n",
    "                    combined_idx.append([memory, repeat_idx[i]+(repeat_length*2)-1])\n",
    "                count = 0\n",
    "            else:\n",
    "                if repeat_count == 2:\n",
    "                    combined_idx.append([repeat_idx[i], repeat_idx[i]+(repeat_length*2)-1])\n",
    "                    count = 0\n",
    "\n",
    "    return(combined_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fb8a4f2-6521-4cb5-aa70-595d3750a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is just a rehash of the above homopolymer function \n",
    "#that works with repeats instead\n",
    "\n",
    "def repeat_analysis(alignres, repeat_length = 3, repeat_count = 2):\n",
    "    \"\"\"\n",
    "    A function that analyses the error rates due to repeats.\n",
    "    It separately analyses error rates within the repeat and\n",
    "    error rate directly one base downstream of the repeat.\n",
    "    Returns two lists, [repeat_errors], [post-repeat errors]\n",
    "    which each contain four variables representing the error rate\n",
    "    of: transitions, transversions, insertions, deletions.\n",
    "\n",
    "    alignres - AlignResults object\n",
    "    repeating_element - int, the size of the repeating element,\n",
    "                        e.g., 2 = \"ATAT\" - \"AT\",\n",
    "                              3 = \"ATGATG\" - \"ATG\".\n",
    "                        Default = 3\n",
    "    repeat_count - int, minimum times repeating element must\n",
    "                   be encountered. E.g., if 3 then\n",
    "                   \"ATGATG\" won't be taken into account but\n",
    "                   \"ATGATGATG\" will. Default = 2\n",
    "    \"\"\"\n",
    "\n",
    "    repeat_indexes = repeat_finder(alignres, repeat_length, repeat_count)\n",
    "    sequ = alignres.alignment[0]\n",
    "\n",
    "    #here errors follows [transitions,transverstions,\n",
    "    #                     insertions, deletions]\n",
    "    repeat_errors = [0,0,0,0]\n",
    "    post_errors = [0,0,0,0]\n",
    "    repeat_total, post_total = 0,0\n",
    "    for i in repeat_indexes:\n",
    "        for j in range(i[0], i[1]+1):\n",
    "            repeat_total += 1\n",
    "            if j in alignres.index_transitions:\n",
    "                repeat_errors[0] += 1\n",
    "            elif j in alignres.index_transversions:\n",
    "                repeat_errors[1] += 1\n",
    "            #Technically this shouldn't be here as if it works\n",
    "            #correctly you shouldn't be getting any insertions\n",
    "            #mid repeat.\n",
    "            elif j in alignres.index_insertions:\n",
    "                repeat_errors[2] += 1\n",
    "            elif j in alignres.index_deletions:\n",
    "                repeat_errors[3] += 1\n",
    "\n",
    "        post_total += 1\n",
    "        if (i[1]+1) in alignres.index_transitions:\n",
    "            post_errors[0] += 1\n",
    "        elif (i[1]+1) in alignres.index_transversions:\n",
    "            post_errors[1] += 1\n",
    "        elif (i[1]+1) in alignres.index_insertions:\n",
    "            post_errors[2] += 1\n",
    "        elif (i[1]+1) in alignres.index_deletions:\n",
    "            post_errors[3] += 1\n",
    "\n",
    "    if repeat_total == 0:\n",
    "        repeat_errors = [0,0,0,0]\n",
    "        post_errors = [0,0,0,0]\n",
    "    else:\n",
    "        #Divide every element by the total\n",
    "        repeat_errors[:] = [x / repeat_total for x in repeat_errors]\n",
    "        post_errors[:] = [x / post_total for x in post_errors]\n",
    "\n",
    "    return (repeat_errors, post_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d626ba4e-cdb9-418c-815e-9a0f47e49669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_analysis_multiple(alignres_list, repeat_length = 3, repeat_count = 2):\n",
    "    \"\"\"\n",
    "    A function that analyses the error rates due to repeats.\n",
    "    It separately analyses error rates within the repeat and\n",
    "    error rate directly one base downstream of the repeat.\n",
    "    Returns two lists, [repeat_errors], [post-repeat errors]\n",
    "    which each contain four variables representing the error rate\n",
    "    of: transitions, transversions, insertions, deletions.\n",
    "\n",
    "    alignres_list - List of AlignResults objects\n",
    "    repeating_element - int, the size of the repeating element,\n",
    "                        e.g., 2 = \"ATAT\" - \"AT\",\n",
    "                              3 = \"ATGATG\" - \"ATG\".\n",
    "                        Default = 3\n",
    "    repeat_count - int, minimum times repeating element must\n",
    "                   be encountered. E.g., if 3 then\n",
    "                   \"ATGATG\" won't be taken into account but\n",
    "                   \"ATGATGATG\" will. Default = 2\n",
    "    \"\"\"\n",
    "\n",
    "    repeat_errors = [0,0,0,0]\n",
    "    post_errors = [0,0,0,0]\n",
    "    count = 0\n",
    "    for i in alignres_list:\n",
    "        count += 1\n",
    "        if i != None:\n",
    "            x,y = repeat_analysis(i, repeat_length, repeat_count)\n",
    "            #Adding to existing errors\n",
    "            for j in range(0,4):\n",
    "                repeat_errors[j] += x[j]\n",
    "                post_errors[j] += y[j]\n",
    "\n",
    "    repeat_errors[:] = [x / count for x in repeat_errors]\n",
    "    post_errors[:] = [x / count for x in post_errors]\n",
    "\n",
    "    return(repeat_errors, post_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acd3be47-66f1-4de2-bbd8-b61a6bf02f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_sliding_window(alignres, windowsize = 100, step = 50, group = True, rate = False):\n",
    "    \"\"\"\n",
    "    A function that performs a GC sliding window analysis on an alignment.\n",
    "    For each window, it calculates GC content, as well as the number\n",
    "    of transitions,transversion,insertsions,deletions or their rates\n",
    "    if rate = True.\n",
    "    Returns a list of lists. [GC content, [error rates]]\n",
    "    alignres - An AlignResults object.\n",
    "    windowsize - int, the bp size of the sliding window. Default = 100\n",
    "               - float, relative size to the length of the alignment\n",
    "    step - int, the bp size of each step of the window. I.e., how\n",
    "           many bases the window moves by in each iteration.\n",
    "         - float, relative size to the length of the alignment\n",
    "           Default = 50\n",
    "    group - boole, whether or not to treat indels greater than one bp in\n",
    "            size as a single indel or multiple indels. Default = True\n",
    "    rate - boole, whether to return the error rate per base instead of\n",
    "           the number of errors. Default = False\n",
    "    \"\"\"\n",
    "\n",
    "    #If given as a percentage\n",
    "    if windowsize < 1:\n",
    "        windowsize = int(windowsize*len(alignres))\n",
    "    if step < 1:\n",
    "        step = int(step*len(alignres))\n",
    "\n",
    "    #These two values are needed to treat large indels as just one indel\n",
    "    prev_ins, prev_del = False, False \n",
    "    gc_list = []\n",
    "    i = 0\n",
    "    #If group is True, \"A---A\" is one gap\n",
    "    #If group is False, \"A---A\" is three gaps\n",
    "    if group == True:\n",
    "        while (i + windowsize) < len(alignres):\n",
    "            frac = gc_fraction(alignres.alignment[0][i:i+windowsize])\n",
    "            gc_errors = [0,0,0,0]\n",
    "            for j in range(i, i+windowsize):\n",
    "                if j in alignres.index_transitions:\n",
    "                    gc_errors[0] += 1\n",
    "                    prev_ins, prev_del = False, False \n",
    "                elif j in alignres.index_transversions:\n",
    "                    gc_errors[1] += 1\n",
    "                    prev_ins, prev_del = False, False \n",
    "                elif j in alignres.index_insertions:\n",
    "                    if prev_ins == True:\n",
    "                        continue\n",
    "                    else:\n",
    "                        gc_errors[2] += 1\n",
    "                        prev_ins, prev_del = True, False \n",
    "                elif j in alignres.index_deletions:\n",
    "                    if prev_del == True:\n",
    "                        continue\n",
    "                    else:\n",
    "                        gc_errors[3] += 1\n",
    "                        prev_ins, prev_del = False, True\n",
    "            i += step\n",
    "            if rate == True:\n",
    "                gc_errors = (np.array(gc_errors) / windowsize).tolist()\n",
    "            gc_list.append([frac, gc_errors])\n",
    "    else:\n",
    "        while (i + windowsize) < len(alignres):\n",
    "            frac = gc_fraction(alignres.alignment[0][i:i+windowsize])\n",
    "            gc_errors = [0,0,0,0]\n",
    "            for j in range(i, i+windowsize):\n",
    "                if j in alignres.index_transitions:\n",
    "                    gc_errors[0] += 1\n",
    "                elif j in alignres.index_transversions:\n",
    "                    gc_errors[1] += 1\n",
    "                elif j in alignres.index_insertions:\n",
    "                    gc_errors[2] += 1\n",
    "                elif j in alignres.index_deletions:\n",
    "                    gc_errors[3] += 1\n",
    "            i += step\n",
    "            if rate == True:\n",
    "                gc_errors = (np.array(gc_errors) / windowsize).tolist()\n",
    "            gc_list.append([frac, gc_errors])\n",
    "\n",
    "    return(gc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2337a42f-d9b5-4bc5-87d4-c4675352d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_grouper(gc_list, blocks = 5):\n",
    "    \"\"\"\n",
    "    A function which groups the results from the\n",
    "    gc_sliding_window function into a defined amount of blocks.\n",
    "    For example if blocks = 5, the results from the function\n",
    "    will be divided into 0.05 GC blocks. [[0-0.05, error rates],\n",
    "    [0.05-0.10, error rates]...]\n",
    "    gc_list - list, A list generated from the gc_sliding_window\n",
    "                    function\n",
    "    blocks - int, The size of each GC block.\n",
    "    \"\"\"\n",
    "\n",
    "    final_list = []\n",
    "    i = 0.00\n",
    "    while i < 1:\n",
    "        count = 0\n",
    "        results = np.array([0,0,0,0],dtype='d')\n",
    "        for j in gc_list:\n",
    "            if j[0] < (i + (blocks/100)) and j[0] >= i:\n",
    "                results += np.array(j[1])\n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            avg = (results / count).tolist()\n",
    "            final_list.append([round(i,2), avg])\n",
    "        else:\n",
    "            final_list.append([round(i,2),[0,0,0,0]])\n",
    "\n",
    "        i += (blocks/100)\n",
    "    return(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0de66121-2f85-4377-b517-fae4b0f624ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_sliding_window_multiple(alignres, windowsize = 100, step = 50, group = True, rate = False):\n",
    "    \"\"\"\n",
    "    A function that performs a GC sliding window analysis on a list \n",
    "    of alignments. For each window, it calculates GC content, as \n",
    "    well as the number of transitions,transversion,insertsions,deletions\n",
    "    or their rates if rate = True.\n",
    "    Returns a list of lists. [GC content, [error rates]]\n",
    "    alignres - An AlignResults object.\n",
    "    windowsize - int, the bp size of the sliding window. Default = 100\n",
    "               - float, relative size to the length of the alignment\n",
    "    step - int, the bp size of each step of the window. I.e., how\n",
    "           many bases the window moves by in each iteration.\n",
    "         - float, relative size to the length of the alignment\n",
    "           Default = 50\n",
    "    group - boole, whether or not to treat indels greater than one bp in\n",
    "            size as a single indel or multiple indels. Default = True\n",
    "    rate - boole, whether to return the error rate per base instead of\n",
    "           the number of errors. Default = False\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    for i in alignres:\n",
    "        if i != None:\n",
    "            final += (gc_sliding_window(i, windowsize, step, group, rate))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4a57d56-3241-4740-a1db-bac8a4509d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Incredibly messy can be written much better.\n",
    "\n",
    "def area_creator(lst_idx):\n",
    "    \"\"\"\n",
    "    A function that takes in a list of indexes and converts\n",
    "    them into a list of lists, grouping runs (gap of 1) together.\n",
    "    For example, [1,2,3,5,8,9] becomes:\n",
    "    [[1,3],[5],[8,9]]\n",
    "    \"\"\"\n",
    "\n",
    "    final = []\n",
    "    series = False #whether the indexes are one after another\n",
    "    memory = None #record the first element of the series\n",
    "    for i in range(len(lst_idx)):\n",
    "        if i == 0: #skipping first index as comparisons are against -1\n",
    "            continue\n",
    "        if i == len(lst_idx)-1: #Separate section for last index needed\n",
    "            if series == True:\n",
    "                series = False\n",
    "                if (lst_idx[i] - lst_idx[i-1]) == 1:\n",
    "                    if len(final) > 0:\n",
    "                        if memory == final[-1][0]:\n",
    "                            final.pop(-1)\n",
    "                    final.append([memory,lst_idx[i]])\n",
    "                else:\n",
    "                    final.append([lst_idx[i]])\n",
    "        if (lst_idx[i] - lst_idx[i-1]) == 1:\n",
    "            if series == False:\n",
    "                memory = lst_idx[i-1]\n",
    "                series = True\n",
    "            series = True\n",
    "            continue\n",
    "        else:\n",
    "            if series == True:\n",
    "                series = False\n",
    "                if len(final) > 0: #removal of duplicates\n",
    "                    if memory == final[-1][0]:\n",
    "                        final.pop(-1)\n",
    "                final.append([memory,lst_idx[i-1]])\n",
    "                final.append([lst_idx[i]])\n",
    "            else:\n",
    "                final.append([lst_idx[i]])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41371fd2-33cb-44c8-b604-5d9a7adc72f5",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36f8e5-dad4-41f6-a178-5aab905b8200",
   "metadata": {},
   "source": [
    "### Q-score analysis in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e69998d-d14a-4bd4-bd38-8d0160804fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_grouper_qscore(gc_list, blocks = 5):\n",
    "    \"\"\"\n",
    "    A function which groups the results from the\n",
    "    gc_sliding_window function into a defined amount of blocks.\n",
    "    For example if blocks = 5, the results from the function\n",
    "    will be divided into 0.05 GC blocks. [[0-0.05, Q-score],\n",
    "    [0.05-0.10, Q-scire]...]\n",
    "    gc_list - list, A list generated from the gc_sliding_window\n",
    "                    function\n",
    "    blocks - int, The size of each GC block.\n",
    "    \"\"\"\n",
    "\n",
    "    final_list = []\n",
    "    i = 0.00\n",
    "    while i < 1:\n",
    "        count = 0\n",
    "        results = 0\n",
    "        for j in gc_list:\n",
    "            if j[0] < (i + (blocks/100)) and j[0] >= i:\n",
    "                results += j[1]\n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            avg = (results / count)\n",
    "            final_list.append([round(i,2), avg])\n",
    "        else:\n",
    "            final_list.append([round(i,2),0])\n",
    "\n",
    "        i += (blocks/100)\n",
    "    return(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed03ff1-d8d4-4907-a372-f186a389dbca",
   "metadata": {},
   "source": [
    "Below is the main function used to generate the data used for the analysis. It generates many results at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbc0e665-4ebd-445c-ba36-0a2b61f8ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_qscore_analysis(filename, show = True, plength = 3, p_downstream = 0, skip = 0, trim = 0,\n",
    "                        windowsize = 100, step = 50, repeat_length = 3, ignore = 0, r_downstream = 0, limit = 0):\n",
    "    \"\"\"\n",
    "    The main q-score analysis function.\n",
    "    Takes in a fastq file.\n",
    "    Output: Average Q-score, GC homopolymer Q-score, AT homopolymer Q-score,\n",
    "            All hompolymer Q-score, Repeat (size 2) Q-score,\n",
    "            Repeat (size 3) Q-score.\n",
    "    filename - str, filename/location of fastq file.\n",
    "    show - boole, whether to print the results. Default = True\n",
    "    plength - int, minimum homopolymer length. Default = 3\n",
    "    p_downstream - int, how many bases downstream of a homopolymer\n",
    "                   to be included in the homopolymer analysis.\n",
    "                   Default = 0\n",
    "    skip - int, how many bases to exclude from the homopolymer analysis\n",
    "           beginning from the start of the homopolymer. Default = 0\n",
    "    trim - float, proportion of the start and end of each read to be\n",
    "           trimmed, mainly for testing. Default = 0\n",
    "    windowsize - int, the base size of the window for GC analysis.\n",
    "                 Default = 100\n",
    "    step - int, the step size for each shift of the GC window.\n",
    "           Default = 50.\n",
    "    repeat_length - int, the minimum amount of repepating elements\n",
    "                    needed to occur to be classified as either a \n",
    "                    dimer or trimer repeat. Default = 3\n",
    "    ignore - int, how many bases to ignore at the start of the repeat. For \n",
    "             example, a value of 2, with \"ATGATG\", only \"GATG\" is counted.\n",
    "             Defualt = 0\n",
    "    r_downstream - int, how many bases downstream of a trimer or dimer\n",
    "                   repeat to be included in the repeat analysis.\n",
    "                   Default = 0.\n",
    "    limit - int, the minimum base size (larger than) that a read must be,\n",
    "            otherwise it is ignored.\n",
    "            Default = 0.\n",
    "    \"\"\"\n",
    "\n",
    "    datas = []\n",
    "    for seq_record in SeqIO.parse(filename, \"fastq\"):\n",
    "        datas.append(seq_record)\n",
    "    polymer_GC, polymer_AT, avg1 = [],[],[]\n",
    "    gc, repeat_3, repeat_2 = [],[],[]\n",
    "    for i in datas:\n",
    "        if len(i) > limit:\n",
    "            x,y = polymer_avg_qscore(i, plength, p_downstream, skip, trim, type = \"GC\")\n",
    "            polymer_GC.append(x)\n",
    "            avg1.append(y)\n",
    "            x,y = polymer_avg_qscore(i, plength, p_downstream, skip, trim, type = \"AT\")\n",
    "            polymer_AT.append(x)\n",
    "            avg1.append(y)\n",
    "            gc.append(analyse_gc_qscore(i, windowsize, step, trim))\n",
    "            repeat_3.append(find_repeats_qscore(i, repeat_length, ignore, r_downstream, trim))\n",
    "            repeat_2.append(find_repeats_qscore(i, 2, ignore, r_downstream, trim))\n",
    "\n",
    "    if show == True:\n",
    "        print('Results:')\n",
    "        print('Average Q-score: ', (sum(avg1) / len(avg1)))\n",
    "        print('GC Homopolymer Q-score: ', (sum(polymer_GC)/len(polymer_GC)))\n",
    "        print('AT Homopolymer Q-score: ', (sum(polymer_AT)/len(polymer_AT)))\n",
    "        print('All Homopolymer Q-score: ', ((sum(polymer_GC) + sum(polymer_AT)) / (len(polymer_GC) + len(polymer_AT))))\n",
    "        print('Repeat Q-score (size 3): ', (sum(repeat_3) / len(repeat_3)))\n",
    "        print('Repeat Q-score (size 2): ', (sum(repeat_2) / len(repeat_2)))\n",
    "        \n",
    "    return([(sum(avg1) / len(avg1)), (sum(polymer_GC)/len(polymer_AT)),(sum(polymer_AT)/len(polymer_AT)),\n",
    "            ((sum(polymer_GC) + sum(polymer_AT)) / (len(polymer_GC) + len(polymer_AT))),\n",
    "            (sum(repeat_3) / len(repeat_3)), (sum(repeat_2) / len(repeat_2)), gc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b19c27-fc89-4b90-9fe3-d71e28016b46",
   "metadata": {},
   "source": [
    "Below is an example as to how the analysis was performed and how the data was exported into csv files using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6189c6ad-85ed-44e5-b743-bc59dd5d26fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR9772230_GC_Q.csv\n",
      "Results:\n",
      "Average Q-score:  32.640651041187944\n",
      "GC Homopolymer Q-score:  24.949135939432747\n",
      "AT Homopolymer Q-score:  29.81854476736324\n",
      "All Homopolymer Q-score:  27.383840353397993\n",
      "Repeat Q-score (size 3):  21.57360551058299\n",
      "Repeat Q-score (size 2):  31.069804905809256\n",
      "ERR9772276_GC_Q.csv\n",
      "Results:\n",
      "Average Q-score:  31.070080935995822\n",
      "GC Homopolymer Q-score:  27.570888953801735\n",
      "AT Homopolymer Q-score:  24.85735097816374\n",
      "All Homopolymer Q-score:  26.214119965982736\n",
      "Repeat Q-score (size 3):  18.128789708046245\n",
      "Repeat Q-score (size 2):  30.082885578733812\n",
      "SRR27883880_GC_Q.csv\n",
      "Results:\n",
      "Average Q-score:  24.91477623316758\n",
      "GC Homopolymer Q-score:  18.912241297956516\n",
      "AT Homopolymer Q-score:  22.480942252549816\n",
      "All Homopolymer Q-score:  20.696591775253168\n",
      "Repeat Q-score (size 3):  15.265493085550434\n",
      "Repeat Q-score (size 2):  23.3963674639759\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "count = 1\n",
    "for i in [\"NextSeq_Human/ERR9772230.fastq\",\"NextSeq_Human/ERR9772276.fastq\",\n",
    "          \"NextSeq_Human/SRR27883880.fastq\"]:\n",
    "    #To name files automaitcally\n",
    "    name, write = \"\", False\n",
    "    for j in i:\n",
    "        if j == \"/\":\n",
    "            write = True\n",
    "            continue\n",
    "        if j == \".\":\n",
    "            break\n",
    "        if write == True:\n",
    "            name += j\n",
    "    name += \"_GC_Q.csv\"\n",
    "    print(name)\n",
    "    if first == True:\n",
    "        x = avg_qscore_analysis(i, limit = 50)\n",
    "        first = False\n",
    "        avg_q = pd.DataFrame([[name] + x[0:6]], columns = [\"Run\",\"Avg_Q_score\",\"GC_polymer\",\"AT_polymer\",\"All_polymer\",\"Repeat_Size_2\",\"Repeat_Size_3\"])\n",
    "        y = []\n",
    "        for i in x[6]:\n",
    "            y += i\n",
    "        y = gc_grouper_qscore(y,5)\n",
    "        gc_q_df = pd.DataFrame(y, columns = [\"GC\", \"Q_score\"])\n",
    "        gc_q_df.to_csv(name, encoding = 'utf-8', index = False)\n",
    "    else:\n",
    "        x = avg_qscore_analysis(i, limit = 50)\n",
    "        avg_q.loc[count] = [name] + x[0:6]\n",
    "        count += 1\n",
    "        y = []\n",
    "        for i in x[6]:\n",
    "            y += i\n",
    "        y = gc_grouper_qscore(y,5)\n",
    "        gc_q_df = pd.DataFrame(y, columns = [\"GC\", \"Q_score\"])\n",
    "        gc_q_df.to_csv(name, encoding = 'utf-8', index = False)\n",
    "\n",
    "avg_q.to_csv(\"Avg_Q_NextSeq_Human.csv\", encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28757a2-5abf-4ea5-962f-af07b8e30683",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece13603-699d-4139-8393-399406e206d2",
   "metadata": {},
   "source": [
    "### Indel/substitution analysis in one function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45e9f1-636c-423c-9553-08e103d12ef4",
   "metadata": {},
   "source": [
    "This function here was introduced late and aimed to analyses whther the position of the read affected the error rate. \\\n",
    "Blocks below is into howmany segments the read is divided into. \\\n",
    "100 means 0-1%, 1-2%, 2-3%..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6894803a-cffc-4584-99a1-74e1e5973a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_analysis(alignres, blocks = 100, group = True):\n",
    "\n",
    "    size = len(alignres) / blocks\n",
    "    prev_ins, prev_del = False, False \n",
    "    error_list = []\n",
    "    i,region = 0, 0\n",
    "    #If group is True, \"A---A\" is one gap\n",
    "    #If group is False, \"A---A\" is three gaps\n",
    "    if group == True:\n",
    "        while (i + size) < len(alignres):\n",
    "            errors = [0,0,0,0]\n",
    "            x = int(round(i + size))\n",
    "            for j in range(i, x):\n",
    "                if j in alignres.index_transitions:\n",
    "                    errors[0] += 1\n",
    "                    prev_ins, prev_del = False, False \n",
    "                elif j in alignres.index_transversions:\n",
    "                    errors[1] += 1\n",
    "                    prev_ins, prev_del = False, False \n",
    "                elif j in alignres.index_insertions:\n",
    "                    if prev_ins == True:\n",
    "                        continue\n",
    "                    else:\n",
    "                        errors[2] += 1\n",
    "                        prev_ins, prev_del = True, False \n",
    "                elif j in alignres.index_deletions:\n",
    "                    if prev_del == True:\n",
    "                        continue\n",
    "                    else:\n",
    "                        errors[3] += 1\n",
    "                        prev_ins, prev_del = False, True\n",
    "            i += size\n",
    "            i = int(round(i))\n",
    "            region += 1/blocks\n",
    "            region = round(region,3)\n",
    "            errors = (np.array(errors) / size).tolist()\n",
    "            error_list.append(errors)\n",
    "    else:\n",
    "        while (i + size) < len(alignres):\n",
    "            errors = [0,0,0,0]\n",
    "            for j in range(i, int(round(i+size))):\n",
    "                if j in alignres.index_transitions:\n",
    "                    errors[0] += 1\n",
    "                elif j in alignres.index_transversions:\n",
    "                    errors[1] += 1\n",
    "                elif j in alignres.index_insertions:\n",
    "                    errors[2] += 1\n",
    "                elif j in alignres.index_deletions:\n",
    "                    errors[3] += 1\n",
    "            i += size\n",
    "            i = int(round(i))\n",
    "            region += 1/blocks\n",
    "            region = round(region,3)\n",
    "            errors = (np.array(errors) / size).tolist()\n",
    "            error_list.append(errors)\n",
    "\n",
    "    while len(error_list) < blocks:\n",
    "        error_list.append([0,0,0,0])\n",
    "    while len(error_list) > blocks:\n",
    "        del error_list[-1]\n",
    "    return(np.array(error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "888e196f-4db7-4b28-9c0c-3a11c02224b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_analysis_multiple(res_list, blocks = 100, group = True):\n",
    "\n",
    "    first = True\n",
    "    count = 0\n",
    "    for i in res_list:\n",
    "        if i != None:\n",
    "            count += 1\n",
    "            if first:\n",
    "                errors = regional_analysis(i, blocks, group)\n",
    "                first = False\n",
    "            else:\n",
    "                errors += regional_analysis(i,blocks,group)\n",
    "    errors = errors / count\n",
    "    return(errors)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f5f643f-7ab7-4b7e-a75d-4948ca592292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_value_error_alignment(x):\n",
    "    '''\n",
    "    A function to deal with empty alignments when comparing\n",
    "    lengths\n",
    "    '''\n",
    "    try:\n",
    "        len(x[0])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ba0618c-37a4-4dbe-a551-85437e53fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_analysis(filename, filetype = \"sam\", drop_last_insertion = True, drop_limit = 20, cutoff = 0.2, limit = 50):\n",
    "\n",
    "    alignments = Align.parse(filename, filetype)\n",
    "    align_list = parse_iterator(alignments)\n",
    "    if limit != 0:\n",
    "        align_list = [x for x in align_list if len_value_error_alignment(x) > limit]\n",
    "    res_list = []\n",
    "    for i in align_list:\n",
    "        res_list.append(align_analysis(i, cutoff = cutoff))\n",
    "        \n",
    "    insertions = {}\n",
    "    deletions = {}\n",
    "    for i in res_list:\n",
    "        if i != None:\n",
    "            temp = indel_size_analysis(i, drop_last_insertion, drop_limit)\n",
    "            for j in temp[0]:\n",
    "                if j not in insertions:\n",
    "                    insertions[j] = 1\n",
    "                else:\n",
    "                    insertions[j] += 1\n",
    "            for j in temp[1]:\n",
    "                if j not in deletions:\n",
    "                    deletions[j] = 1\n",
    "                else:\n",
    "                    deletions[j] += 1\n",
    "    return(insertions, deletions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6bdd5eba-f28a-4773-88d3-a92e22471a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_combiner(dic1, dic2):\n",
    "\n",
    "    for i in dic2:\n",
    "        if i not in dic1:\n",
    "            dic1[i] = dic2[i]\n",
    "        else:\n",
    "            dic1[i] += dic2[i]\n",
    "    return(dic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "773c779c-8097-44a1-9452-0dde927b6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_dict_df(dic):\n",
    "\n",
    "    final_list = []\n",
    "    for i in sorted(dic):\n",
    "        final_list.append([i, dic[i]])\n",
    "    final_df = pd.DataFrame(final_list, columns = [\"Size\", \"Count\"])\n",
    "    return(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8dd10-c652-439f-bd69-54af96f67cf7",
   "metadata": {},
   "source": [
    "Below is the main function for analysis. Outputs pandas dataframes that can later be converted into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d44e7bfb-3396-43ab-8eba-9d4ce9d103b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indel_substitution_analysis(filename, filetype = \"sam\", group = True, polymer_length = 3, repeat_length = 3,\n",
    "                                repeat_count = 3, windowsize = 100, step = 50, rate = True, gc_blocks = 5,\n",
    "                                reg_blocks = 10, cutoff = None, limit = 0):\n",
    "    \"\"\"\n",
    "    outputs three dataframes; regular errors, gc errors and regional errors\n",
    "    filename - str, name/location of file\n",
    "    filetype - \"sam\" or \"bam\", type of alignment file used.\n",
    "    group - True or False, whether to count large indels as one. E.g\n",
    "            If true \"A---A\" counts as 1 gap, If false -> 3 gaps.\n",
    "            Default = True\n",
    "    polymer_length - int, minimum homopolymer length. Default = 3\n",
    "    repeat_length - int or list, the size fo the repeating element.\n",
    "                    Default = 3\n",
    "    repeat_count - int or list, minimum number of repeating elements to be\n",
    "                    considered a repeat. Default = 3\n",
    "    windowsize - int, the base size of the window for GC analysis. Default = 100\n",
    "    step - int, the step size for the sliding GC window. Default = 50\n",
    "    rate - boole, whether to return the error rate per base instead of\n",
    "           the number of errors. Default = True\n",
    "    gc_blocks - int, the size of the blocks that GC is grouped into.\n",
    "                E.g., if 5 then 0.01, 0.02... 0.05, will all be grouped as 0.05.\n",
    "                Default = 5.\n",
    "    reg_blocks - int, the size of each block for comparing regional error rates.\n",
    "                 Default = 10\n",
    "    cutoff - float (0-1), if error rate is higher than cuttoff, it is ignored.\n",
    "             Default = 0 \n",
    "    limit - int, the minimum base size (larger than) that a read must be,\n",
    "            otherwise it is ignored.\n",
    "            Default = 0.\n",
    "    \"\"\"\n",
    "    alignments = Align.parse(filename, filetype)\n",
    "    align_list = parse_iterator(alignments)\n",
    "    if limit != 0:\n",
    "        align_list = [x for x in align_list if len_value_error_alignment(x) > limit]\n",
    "    res_list = []\n",
    "    for i in align_list:\n",
    "        res_list.append(align_analysis(i, cutoff = cutoff))\n",
    "\n",
    "    flat_errors = flat_error_analysis(res_list, group)\n",
    "    polymer = homopolymer_analysis_multiple(res_list, polymer_length, bases = None)\n",
    "    polymer_AT = homopolymer_analysis_multiple(res_list, polymer_length, bases = \"AT\")\n",
    "    polymer_GC = homopolymer_analysis_multiple(res_list, polymer_length, bases = \"GC\")\n",
    "\n",
    "    df = pd.DataFrame(np.array([[\"flat\"] + flat_errors, [\"all_polymers\"] + polymer[0], [\"post_all_polymer\"] + polymer[1],\n",
    "                               [\"AT_polymers\"] + polymer_AT[0], [\"post_AT_polymers\"] + polymer_AT[1],\n",
    "                               [\"GC_polymers\"] + polymer_GC[0], [\"post_GC_polymers\"] + polymer_GC[1]]),\n",
    "                                columns=['name','transitions', 'transversions', 'insertions','deletions'])\n",
    "       \n",
    "    if type(repeat_length) == list:\n",
    "        for i in range(len(repeat_length)):\n",
    "            x = repeat_analysis_multiple(res_list, repeat_length[i], repeat_count[i])\n",
    "            df.loc[len(df.index)] = [\"Repeat \" + str(repeat_length[i]) + \", \" + str(repeat_count[i])] + x[0]\n",
    "            df.loc[len(df.index)] = [\"Post_Repeat \" + str(repeat_length[i]) + \", \" + str(repeat_count[i])] + x[1]\n",
    "    else:\n",
    "        x = repeat_analysis_multiple(res_list, repeat_length, repeat_count)\n",
    "        df.loc[len(df.index)] = [\"Repeat \" + str(repeat_length) + \", \" + str(repeat_count)] + x[0]\n",
    "        df.loc[len(df.index)] = [\"Post_Repeat \" + str(repeat_length) + \", \" + str(repeat_count)] + x[1]\n",
    "\n",
    "    gc2 = []\n",
    "    gc = gc_grouper(gc_sliding_window_multiple(res_list, windowsize, step, group, rate), gc_blocks)\n",
    "    # converting to a single list instead of having gc content and error rates as separate lists\n",
    "    for i in gc:\n",
    "        gc2.append([i[0]] + i[1])\n",
    "    gc_df = pd.DataFrame(gc2, columns = [\"GC_content\", \"transitions\", \"transversions\", \"insertions\", \"deletions\"])\n",
    "    \n",
    "    regional = regional_analysis_multiple(res_list, reg_blocks, group)\n",
    "    regional_blocks = [(i/reg_blocks + 1/reg_blocks) for i in range(reg_blocks)]\n",
    "    reg_df = pd.DataFrame(regional, columns = ['transitions', 'transversions', 'insertions','deletions'])  \n",
    "    reg_df.insert(0, \"block\", regional_blocks, True)\n",
    "    return(df, gc_df, reg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f28b0-5df0-4f17-8462-8ecd837ac43b",
   "metadata": {},
   "source": [
    "Below is a function that simply looks at how homopolymer length affects error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e479724-4468-4eb2-bc7f-57e0d5bb7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating another function as the first one is getting to messy\n",
    "#Parsing the alignment isn't very computationally expensive so \n",
    "#it's not too bad to have them separate.\n",
    "def homopolymer_length_analysis(filename, filetype = \"sam\", polymer_length = [3,12], bases = None, \n",
    "                                specific = False, cutoff = None, limit = 0):\n",
    "\n",
    "    alignments = Align.parse(filename, filetype)\n",
    "    align_list = parse_iterator(alignments)\n",
    "    if limit != 0:\n",
    "        align_list = [x for x in align_list if len_value_error_alignment(x) > limit]\n",
    "    res_list = []\n",
    "    for i in align_list:\n",
    "        res_list.append(align_analysis(i, cutoff = cutoff))\n",
    "\n",
    "    pol_list = []\n",
    "    for i in range(polymer_length[0],polymer_length[1]+1):\n",
    "        #Ignoring post errors for this one, however insertion rate\n",
    "        # in post errors is technically the actual insertion rate in polymer.\n",
    "        x,y = homopolymer_analysis_multiple(res_list, i, bases, specific)\n",
    "        x[2] = y[2]\n",
    "        pol_list.append([i] + x)\n",
    "\n",
    "    pol_df = pd.DataFrame(pol_list, columns = [\"homopolymer_size\",\"transitions\", \"transversions\", \"insertions\", \"deletions\"])\n",
    "    return(pol_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4c08ab7-30c8-434e-9140-8dbca5f8a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sum_errors(df):\n",
    "    sums = []\n",
    "    for i in range(df.shape[0]):\n",
    "        sums.append(sum(df.iloc[i][1:5]))\n",
    "    df.insert(5, \"total_error_rate\", sums, True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0986a53-9350-409d-9d2d-3e313c3509e8",
   "metadata": {},
   "source": [
    "Below is an example of how the above functions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7bdb6ed-5f40-4942-8f5d-1e5e95e0c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NextSeq_Human/ERR9772230.sam\n",
      "NextSeq_Human/ERR9772276.sam\n",
      "NextSeq_Human/SRR24436571.sam\n",
      "NextSeq_Human/SRR27883880.sam\n",
      "NextSeq_Human/SRR28293697.sam\n"
     ]
    }
   ],
   "source": [
    "ins_size_list, del_size_list = [],[]\n",
    "for i in [\"NextSeq_Human/ERR9772230.sam\",\"NextSeq_Human/ERR9772276.sam\",\"NextSeq_Human/SRR24436571.sam\",\n",
    "          \"NextSeq_Human/SRR27883880.sam\",\"NextSeq_Human/SRR28293697.sam\"]:\n",
    "    print(i)\n",
    "    sizes = size_analysis(i, cutoff = 0.2, limit = 50)\n",
    "    ins_size_list.append(sizes[0])\n",
    "    del_size_list.append(sizes[1])\n",
    "    analyses = indel_substitution_analysis(i, repeat_length = [3,2], repeat_count = [3,3], cutoff = 0.2, limit = 50)\n",
    "    pol_analysis = homopolymer_length_analysis(i, specific = False, limit = 50, cutoff = 0.2)\n",
    "    \n",
    "    \n",
    "    name, write = \"\", False\n",
    "    #To name files automaitcally\n",
    "    for j in i:\n",
    "        if j == \"/\":\n",
    "            write = True\n",
    "            continue\n",
    "        if j == \".\":\n",
    "            break\n",
    "        if write == True:\n",
    "            name += j\n",
    "            \n",
    "    analyses[0].to_csv(name + \"_errors.csv\", encoding='utf-8', index=False)\n",
    "    analyses[1].to_csv(name + \"_GC.csv\", encoding='utf-8', index=False)\n",
    "    analyses[2].to_csv(name + \"_regions.csv\", encoding='utf-8', index=False)\n",
    "    pol_analysis.to_csv(name + \"_hompolymers.csv\", encoding='utf-8', index=False)\n",
    "    \n",
    "\n",
    "insertions = ins_size_list[0]\n",
    "for i in ins_size_list[1:]:\n",
    "    insertions = dictionary_combiner(insertions,i)\n",
    "deletions = del_size_list[0]\n",
    "for i in del_size_list[1:]:\n",
    "    deletions = dictionary_combiner(deletions, i)\n",
    "\n",
    "size_dict_df(insertions).to_csv(\"NextSeq_Human_Insertion_Sizes.csv\", encoding='utf-8', index=False)\n",
    "size_dict_df(deletions).to_csv(\"NextSeq_Human_Deletion_Sizes.csv\", encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f67ea2-d27d-4b8a-8428-a11d08151368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68b8c5-6a1c-4ea1-8f04-13bda852a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[2].to_csv(\"test.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129c3e8-64a0-454c-a5dc-da34f1747479",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "tfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
